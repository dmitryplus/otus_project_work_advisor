from typing import TypedDict, Annotated
from langchain_core.prompts import PromptTemplate
from langgraph.graph import StateGraph, END

from src.rag_service import RAGService
from src.llm_service import LLMService
from src.ocr_service import OCRService
from src.prompt_service import PromptService


# --- –°–æ—Å—Ç–æ—è–Ω–∏–µ –≥—Ä–∞—Ñ–∞ ---
class GraphState(TypedDict):
    query: str
    relevants: list
    context: str
    response: str
    image_data: str
    prompt_template: PromptTemplate


# --- –£–∑–ª—ã –≥—Ä–∞—Ñ–∞ ---
def retrieve_rag_node(state: GraphState) -> dict:
    """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é RAG."""
    print("üîç –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

    if not state["query"]:
        return {"response": "‚ùå –ó–∞–ø—Ä–æ—Å –ø—É—Å—Ç. –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø–æ–∏—Å–∫."}

    rag_service = RAGService()
    relevants = rag_service.search_relevant_documents(state["query"], top_k=3)

    if not relevants:
        return {"response": "‚ùå –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã."}

    context = rag_service.format_context(relevants)

    return {
        "relevants": relevants,
        "context": context,
    }


def decide_to_generate(state: GraphState) -> str:
    """–†–µ—à–∞–µ—Ç, –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –ª–∏ –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–ª–∏ –∑–∞–≤–µ—Ä—à–∏—Ç—å –≥—Ä–∞—Ñ."""
    if state["response"]:
        return "end"
    return "generate"


def generate_node(state: GraphState) -> dict:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ '–ü–æ–¥—Ä–æ–±–Ω–µ–µ –≤ –∑–∞–¥–∞—á–∞—Ö'."""

    if "prompt_template" not in state or state["prompt_template"] is None:
        return {"response": "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω –∑–∞–ø—Ä–æ—Å–∞."}

    print("üß† –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")

    llm_service = LLMService(model="yandexgpt-lite", prompt_template=state["prompt_template"])
    response = llm_service.generate_response(question=state["query"], context=state["context"], state=state).strip()

    full_response = response
    if state["relevants"]:
        full_response += "\n\nüìö –ü–æ–¥—Ä–æ–±–Ω–µ–µ –≤ –∑–∞–¥–∞—á–∞—Ö:\n"
        for doc in state["relevants"]:
            full_response += f"‚Ä¢ {doc['title']}\n"
            full_response += f"  {doc['url']}\n"

    print("‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω")

    return {"response": full_response}


def ocr_image_node(state: GraphState) -> dict:
    """–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ query."""
    if not state["image_data"]:
        return {}

    print("üñºÔ∏è –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏...")
    ocr_service = OCRService()

    try:
        base64_data = state["image_data"].split(",")[1] if state["image_data"].startswith("data:image") else state["image_data"]
        recognized_text = ocr_service.analyze_image(base64_data)

        if not recognized_text or not recognized_text.strip():
            return {"response": "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏."}

        return {"query": recognized_text}

    except Exception as e:
        return {"response": f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}"}


def route_image_or_query(state: GraphState) -> str:
    """–†–µ—à–∞–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–ª–∏ —Å—Ä–∞–∑—É –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç—å –∫ –ø–æ–∏—Å–∫—É."""
    if state["image_data"]:
        return "ocr"
    return "retrieve"


def init_prompt_template_node(state: GraphState) -> dict:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è prompt_template –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞–ª–∏—á–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
    template_path = 'prompts/answer_from_documents.txt'
    if state["image_data"]:
        template_path = 'prompts/text_from_image_to_query.txt'

    prompt_service = PromptService(template_path=template_path)
    prompt_template = prompt_service.get_prompt_template()

    print("üìù –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —à–∞–±–ª–æ–Ω–∞ –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∏...")
    return {"prompt_template": prompt_template}


# --- –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ ---
class GraphService:
    def __init__(self):
        self.app = self._compile_graph()

    def _compile_graph(self):
        workflow = StateGraph(GraphState)

        workflow.add_node("ocr", ocr_image_node)
        workflow.add_node("retrieve", retrieve_rag_node)
        workflow.add_node("init_prompt", init_prompt_template_node)
        workflow.add_node("generate", generate_node)

        workflow.set_conditional_entry_point(
            route_image_or_query,
            {
                "ocr": "ocr",
                "retrieve": "retrieve"
            }
        )

        workflow.add_edge("ocr", "retrieve")

        workflow.add_conditional_edges(
            "retrieve",
            decide_to_generate,
            {
                "generate": "init_prompt",
                "end": END
            }
        )

        workflow.add_edge("init_prompt", "generate")
        workflow.add_edge("generate", END)

        return workflow.compile()

    def invoke(self, inputs: dict) -> dict:
        """–ó–∞–ø—É—Å–∫ –≥—Ä–∞—Ñ–∞ —Å –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º–∏ –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏."""
        return self.app.invoke(inputs)

    def get_mermaid_code(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç Mermaid-–∫–æ–¥ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≥—Ä–∞—Ñ–∞ (–º–æ–∂–Ω–æ –≤—Å—Ç–∞–≤–∏—Ç—å –≤ VS Code –∏–ª–∏ Mermaid Live Editor)."""
        try:
            return self.app.get_graph().draw_mermaid()
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Mermaid-–¥–∏–∞–≥—Ä–∞–º–º—ã: {e}"