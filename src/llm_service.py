import json
import os
import time

from dotenv import load_dotenv
from pathlib import Path

from langchain_community.llms import YandexGPT
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableSequence

from langfuse import get_client, Langfuse
from langfuse.langchain import CallbackHandler

from src.prompt_service import PromptService
from src.semantic_coverage_service import SemanticCoverageService

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ .env —Ñ–∞–π–ª–∞
env_path = Path(__file__).parent.parent / ".env"
load_dotenv(dotenv_path=env_path)


class LLMService:
    def __init__(self,
                 folder_id: str = None,
                 iam_token: str = None,
                 model: str = "yandexgpt",
                 temperature: float = 0.3,
                 max_tokens: int = 500,
                 prompt_template: PromptTemplate = None,
                 langfuse_secret_key: str = None,
                 langfuse_public_key: str = None,
                 langfuse_host: str = None
                 ):

        self.folder_id = folder_id or os.getenv("FOLDER_ID")
        self.iam_token = iam_token or os.getenv("IAM_TOKEN")

        if not self.folder_id:
            raise ValueError("FOLDER_ID –Ω–µ —É–∫–∞–∑–∞–Ω –Ω–∏ –≤ .env, –Ω–∏ –≤ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ö.")
        if not self.iam_token:
            raise ValueError("IAM_TOKEN –Ω–µ —É–∫–∞–∑–∞–Ω –Ω–∏ –≤ .env, –Ω–∏ –≤ –∞—Ä–≥—É–º–µ–Ω—Ç–∞—Ö.")

        self.llm = YandexGPT(
            folder_id=self.folder_id,
            iam_token=self.iam_token,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens
        )

        if not prompt_template:
            self.prompt_service = PromptService()
            self.prompt_template = self.prompt_service.get_prompt_template()
        else:
            self.prompt_template = prompt_template

        # –¶–µ–ø–æ—á–∫–∞: –ø—Ä–æ–º–ø—Ç ‚Üí LLM
        self.chain: RunnableSequence = self.prompt_template | self.llm

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Langfuse
        self.langfuse = Langfuse(
            secret_key=langfuse_secret_key or os.getenv("LANGFUSE_SECRET_KEY"),
            public_key=langfuse_public_key or os.getenv("LANGFUSE_PUBLIC_KEY"),
            host=langfuse_host or os.getenv("LANGFUSE_HOST"),
        )

        self.semantic_coverage_service = SemanticCoverageService()

    def generate_response(self, question: str, context: str, state: dict) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å —É—á—ë—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        –î–æ–±–∞–≤–ª—è–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —á–µ—Ä–µ–∑ Langfuse.
        """

        langfuse = get_client()

        # Generate deterministic trace ID from external system
        external_request_id = "home_request_15"
        predefined_trace_id = Langfuse.create_trace_id(seed=external_request_id)

        langfuse_handler = CallbackHandler()

        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        start_time = time.time()  # –ù–∞—á–∞–ª–æ –≤—Å–µ–π –æ–ø–µ—Ä–∞—Ü–∏–∏

        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –æ—Ü–µ–Ω–∫–∏
        documents = state["relevants"]
        scores = [doc.get("score", 0.0) for doc in documents]

        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        document_count = len(documents)
        average_relevance = sum(scores) / len(scores) if document_count > 0 else 0

        with langfuse.start_as_current_span(
                name="langchain-request-1",
                trace_context={"trace_id": predefined_trace_id}
        ) as span:
            # –§–æ—Ä–º–∏—Ä—É–µ–º input –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            input_data = {
                "question": question,
                "context": context,
                "document_count": document_count,
                "average_relevance": average_relevance,
                "relevance_scores": scores
            }

            print("\n" + "="*60)
            print("üü¢ INPUT:")
            print("="*60)
            print(json.dumps(input_data, ensure_ascii=False, indent=4))
            print("="*60)

            span.update_trace(user_id="user_123", input=input_data)

            # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è LLM-—Ü–µ–ø–æ—á–∫–∏
            llm_start_time = time.time()

            response = self.chain.invoke(
                {
                    "question": question,
                    "context": context
                },
                config={
                    "callbacks": [langfuse_handler],
                    "metadata": {
                        "langfuse_user_id": "random-user",
                        "langfuse_session_id": "random-session",
                        "langfuse_tags": ["random-tag-1", "random-tag-2"],
                        "model_name": "yandexgpt"
                    }
                }
            )

            # –ó–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏ –æ–∫–æ–Ω—á–∞–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è LLM-—Ü–µ–ø–æ—á–∫–∏
            llm_end_time = time.time()
            llm_duration = llm_end_time - llm_start_time  # –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è LLM

            # –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            total_end_time = time.time()
            total_duration = total_end_time - start_time  # –û–±—â–µ–µ –≤—Ä–µ–º—è –æ–ø–µ—Ä–∞—Ü–∏–∏

            semantic_coverage = self.semantic_coverage_service.calculate(context, response, documents)

            output = {
                "response": response,
                "llm_duration_seconds": llm_duration,  # –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã LLM
                "total_duration_seconds": total_duration,
                "document_count": document_count,
                "relevance_scores": scores,
                "average_relevance": average_relevance,
                "semantic_coverage": semantic_coverage
            }

            print("\n" + "üü© OUTPUT:")
            print("="*60)
            print(json.dumps(output, ensure_ascii=False, indent=4))
            print("="*60 + "\n")

        span.update_trace(output=output, )

        return response
